{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/dataset.zip"
      ],
      "metadata": {
        "id": "uAbnsPKepjXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm wandb"
      ],
      "metadata": {
        "id": "mtXRXagyLe6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login b3e9d73d3a833e55685c96273c77feaa9880ae5f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htmrBR_P1Gft",
        "outputId": "9d4f9be7-5ba4-46b2-bf1c-e0d57e2ede67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from operator import itemgetter\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "bjYS9JPoUcQ3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "-2Eb9HCNLoe9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionsDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_dir: str,\n",
        "                 img_size = (224, 224),\n",
        "                 emb_mode = False,\n",
        "                 model = None,\n",
        "                 sample_mode: str = 'triplet',\n",
        "                 count_negatives: int = 2,\n",
        "                 protocol_path='/content/drive/MyDrive',\n",
        "                 prefix = 'train'\n",
        "                 ):\n",
        "        super(EmotionsDataset, self).__init__()\n",
        "\n",
        "        self.dataset_root = dataset_dir\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize(size=img_size),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        self.emb_mode = emb_mode\n",
        "        self.model = model\n",
        "\n",
        "        self.embeddings = list()\n",
        "\n",
        "        self.sample_mode = sample_mode\n",
        "        self.n_count = count_negatives #/val_balanced_protocol.csv\n",
        "\n",
        "        self.protocol_path = os.path.join(protocol_path, f'{prefix}_balanced_protocol.csv')\n",
        "        self.protocol = pd.read_csv(self.protocol_path)\n",
        "\n",
        "        self.images, self.names, self.labels, self.sample_nums = self._load_list(self.dataset_root)\n",
        "        self.classes = {'Neutral': 0, 'Anger': 1, 'Disgust': 2, 'Fear': 3, 'Happiness': 4, 'Sadness': 5, 'Surprise': 6, 'Other': 7}\n",
        "\n",
        "    def _load_list(self, list_root):\n",
        "        samples, sample_names, sample_labels, frame_nums = list(), list(), list(), list()\n",
        "\n",
        "        for path in self.protocol['path'].tolist():\n",
        "            # path, frame, label\n",
        "\n",
        "            sample, name, label, frame_num = self._load_samples_with_labels(path)\n",
        "\n",
        "            samples.append(sample)\n",
        "            sample_names.append(name)\n",
        "            sample_labels.append(int(label))\n",
        "            frame_nums.append(frame_num)\n",
        "\n",
        "        return samples, sample_names, sample_labels, frame_nums\n",
        "\n",
        "    def _load_samples_with_labels(self, path):\n",
        "        name, class_label, frame_num = path.split('/')[-1].replace('.jpeg', '').split('_')\n",
        "\n",
        "        if self.emb_mode:\n",
        "            image = cv2.imread(path)\n",
        "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            image = self.transform(image)\n",
        "\n",
        "            emb = self.model.get_embeddings(image.unsqueeze(0))\n",
        "\n",
        "            self.embeddings.append(emb[0])\n",
        "\n",
        "        return path, name, class_label, frame_num\n",
        "\n",
        "    def extract_embeddings(self, model):\n",
        "        print(f'Extracting embeddings')\n",
        "        self.embeddings = model.get_embeddings(self.images)\n",
        "\n",
        "    def load_image(self, path):\n",
        "        if type(path) != str:\n",
        "            final_list = list()\n",
        "            for p in path:\n",
        "                image = cv2.imread(p)\n",
        "                image = self.transform(image)\n",
        "\n",
        "                final_list.append(image)\n",
        "\n",
        "            return final_list\n",
        "\n",
        "        image = cv2.imread(path)\n",
        "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        #cv2_imshow(image)\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.emb_mode:\n",
        "            return self.embeddings[idx], self.labels[idx], self.names[idx], self.sample_nums[idx]\n",
        "\n",
        "        if self.sample_mode == 'triplet':\n",
        "            anchor, anchor_label, anchor_name = self.load_image(self.images[idx]), self.labels[idx], self.names[idx]\n",
        "            #print(anchor_label)\n",
        "            negative_list = list()\n",
        "\n",
        "            for label in self.classes.values():\n",
        "              if label != anchor_label:\n",
        "                negative_cand = [self.images[i] for i in range(len(self.images)) if\n",
        "                             (self.labels[i] == label)]\n",
        "\n",
        "                negative_list.extend(np.random.choice(negative_cand, self.n_count, replace=False).tolist())\n",
        "\n",
        "\n",
        "            positive_list = [self.images[i] for i in range(len(self.images)) if\n",
        "                             (anchor_name != self.names[i]) and (self.labels[i] == anchor_label)]\n",
        "\n",
        "            # if not positive_list:\n",
        "            #    positive_list = [self.images[i] for i in range(len(self.images)) if\n",
        "            #                  (idx != i) and (self.labels[i] == anchor_label)]\n",
        "\n",
        "            positive = self.load_image(np.random.choice(positive_list, 3, replace=False).tolist())\n",
        "            negative = self.load_image(negative_list)\n",
        "\n",
        "            return anchor, positive, negative, anchor_label\n",
        "\n",
        "        if self.sample_mode == 'arcface':\n",
        "            anchor, anchor_label, anchor_name = self.load_image(self.images[idx]), int(self.labels[idx]), self.names[idx]\n",
        "\n",
        "            return anchor, anchor_label\n",
        "\n",
        "        return self.images[idx], int(self.labels[idx]), self.names[idx], int(self.sample_nums[idx])\n",
        "\n"
      ],
      "metadata": {
        "id": "_t1-ex9NJhXx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArcFaceLoss(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 num_classes: int,\n",
        "                 device: str = 'cuda',\n",
        "                 s: float = 64.0,\n",
        "                 m: float = 0.5,\n",
        "                 eps: float = 1e-6,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super(ArcFaceLoss, self).__init__()\n",
        "\n",
        "        self.in_features = emb_size\n",
        "        self.out_features = num_classes\n",
        "\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "\n",
        "        self.threshold = math.pi - m\n",
        "        self.eps = eps\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(self.out_features, self.in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, target: torch.LongTensor = None) -> torch.Tensor:\n",
        "        cos_theta = F.linear(F.normalize(x), F.normalize(self.weight.to(self.device)))\n",
        "\n",
        "        if not target.numel():\n",
        "            return cos_theta\n",
        "\n",
        "        theta = torch.acos(torch.clamp(cos_theta, -1 + self.eps, 1.0 - self.eps))\n",
        "\n",
        "        one_hot = torch.zeros_like(cos_theta)\n",
        "        one_hot.scatter_(1, target.view(-1, 1).long(), 1)\n",
        "\n",
        "        mask = torch.where(theta > self.threshold, torch.zeros_like(one_hot), one_hot)\n",
        "\n",
        "        logits = torch.cos(torch.where(mask.bool(), theta + self.m, theta))\n",
        "\n",
        "        logits *= self.s\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0, **kwargs):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_distance(x1, x2):\n",
        "        x1 = F.normalize(x1, dim=1)\n",
        "        x2 = F.normalize(x2, dim=1)\n",
        "\n",
        "        cos_sim = (x1 * x2).sum(dim=1)\n",
        "\n",
        "        return 1 - cos_sim\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_stuff(self, pos_scores, neg_scores, prefix):\n",
        "        pos_mean = pos_scores.mean().item()\n",
        "        neg_mean = neg_scores.mean().item()\n",
        "        difference = pos_mean - neg_mean\n",
        "\n",
        "        wandb.log({\n",
        "            f\"{prefix}_pos_mean\": pos_mean,\n",
        "            f\"{prefix}_neg_mean\": neg_mean,\n",
        "            f\"{prefix}_difference\": difference\n",
        "        })\n",
        "\n",
        "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor, prefix: str) -> torch.Tensor:\n",
        "        distance_positive = self.calc_distance(anchor, positive)\n",
        "        distance_negative = self.calc_distance(anchor, negative)\n",
        "\n",
        "        self.log_stuff(distance_positive, distance_negative, prefix)\n",
        "\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "\n",
        "        return losses.mean()\n",
        "\n",
        "\n",
        "class ContrastiveCrossEntropy(nn.Module):\n",
        "    def __init__(self, margin=1.0, **kwargs):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_stuff(self, pos_scores, neg_scores, prefix):\n",
        "        pos_mean = pos_scores.mean().item()\n",
        "        neg_mean = neg_scores.mean().item()\n",
        "        difference = pos_mean - neg_mean\n",
        "\n",
        "        wandb.log({\n",
        "            f\"{prefix}_pos_mean\": pos_mean,\n",
        "            f\"{prefix}_neg_mean\": neg_mean,\n",
        "            f\"{prefix}_difference\": difference\n",
        "        })\n",
        "\n",
        "    def forward(self, vac_emb: torch.Tensor, pos_emb: torch.Tensor, neg_emb: torch.Tensor, prefix: str) -> torch.Tensor:\n",
        "        pos_scores = (vac_emb * pos_emb).sum(dim=1)\n",
        "        neg_scores = (vac_emb * neg_emb).sum(dim=1)\n",
        "\n",
        "        self.log_stuff(pos_scores, neg_scores, prefix)\n",
        "\n",
        "        loss_val = torch.exp(neg_scores + self.margin) - pos_scores\n",
        "\n",
        "        # loss_val = torch.clamp(loss_val, min=1.001, max=2**16)\n",
        "        loss_val[loss_val < 1] = 1\n",
        "\n",
        "        return loss_val.log().mean()"
      ],
      "metadata": {
        "id": "1xeD4YYOUlNE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEmbedder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_path: str,\n",
        "                 embedding_size: int = 512,\n",
        "                 freeze: bool = False,\n",
        "                 device: str = 'cpu',\n",
        "                 normalize: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.base_model = timm.create_model('efficientnet_b1', pretrained=True)\n",
        "        self.base_model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "        self.internal_embedding_size = self.base_model.classifier[0].in_features\n",
        "        self.base_model.classifier = nn.Linear(in_features=self.internal_embedding_size, out_features=embedding_size)\n",
        "        self.normalize = normalize\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = True\n",
        "        self.base_model.classifier.requires_grad_(True)\n",
        "\n",
        "        self.base_model.to(device)\n",
        "\n",
        "    def embed_image(self, image):\n",
        "        return self.base_model(image)\n",
        "\n",
        "    def save(self, model_path):\n",
        "        torch.save(self.base_model.state_dict(), os.path.join(model_path, 'model.pth'))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_embeddings(self, image):\n",
        "\n",
        "        out = self.base_model.forward(image)\n",
        "\n",
        "        if self.normalize:\n",
        "            out = F.normalize(out, dim=1)\n",
        "\n",
        "        return out.cpu().numpy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embed_image(x)\n",
        "\n",
        "        if self.normalize:\n",
        "            embedding = F.normalize(embedding, dim=-1)\n",
        "\n",
        "        return embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "iexPLwcAUqtr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionsTrainer:\n",
        "    def __init__(\n",
        "            self, model, checkpoint_dir,\n",
        "            train_dataloader, dev_dataloader, test_dataloader,\n",
        "            optimizer, optimizer_arc, scheduler, loss,\n",
        "            device='cuda', save_best=False, sample_mode='triplet'\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.save_best = save_best\n",
        "\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.dev_dataloader = dev_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.optimizer_arc = optimizer_arc\n",
        "        self.scheduler = scheduler\n",
        "        self.loss = loss\n",
        "\n",
        "        self.experiment_dir = os.path.join(self.checkpoint_dir,\n",
        "                                           self.model.__class__.__name__+ f'_{datetime.today().strftime(\"%Y_%m_%d\")}')\n",
        "        self.model_dir = os.path.join(self.experiment_dir, 'model')\n",
        "\n",
        "        self.eval_step = 0\n",
        "        self.sample_mode = sample_mode\n",
        "\n",
        "        os.makedirs(self.experiment_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.model_dir), exist_ok=True)\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        with tqdm(total=len(self.train_dataloader)) as pbar:\n",
        "          if self.sample_mode=='triplet':\n",
        "            for i, (anchor, positive, negative, class_label) in enumerate(self.train_dataloader):\n",
        "                anchor = anchor.to(self.device)\n",
        "                anchor_emb = self.model(anchor)\n",
        "\n",
        "                loss = torch.tensor(0.0).to(self.device)\n",
        "\n",
        "                for neg in negative:\n",
        "                    neg = neg.to(self.device)\n",
        "                    negative_emb = self.model(neg)\n",
        "                    for pos in positive:\n",
        "                      pos = pos.to(self.device)\n",
        "                      positive_emb = self.model(pos)\n",
        "\n",
        "                      loss += self.loss(anchor_emb, positive_emb, negative_emb, \"train\")\n",
        "\n",
        "                loss.backward(loss)\n",
        "\n",
        "                self._optimizer_step()\n",
        "\n",
        "                wandb.log({\"train_loss\": loss.item()})\n",
        "\n",
        "                pbar.set_description('Epoch {} - current loss: {:.4f}'.format(epoch, loss.item()))\n",
        "                pbar.update(1)\n",
        "\n",
        "          if self.sample_mode=='arcface':\n",
        "            for i, (anchor, class_label) in enumerate(self.train_dataloader):\n",
        "                anchor = anchor.to(self.device)\n",
        "                class_label = class_label.to(self.device)\n",
        "\n",
        "                anchor_emb = self.model(anchor)\n",
        "\n",
        "                loss = self.loss(anchor_emb, class_label).sum()\n",
        "\n",
        "                loss.backward(loss)\n",
        "\n",
        "                self._optimizer_step()\n",
        "                self.optimizer_arc.step()\n",
        "                self.optimizer_arc.zero_grad()\n",
        "\n",
        "                wandb.log({\"train_loss\": loss.sum().item()})\n",
        "\n",
        "                pbar.set_description('Epoch {} - current loss: {:.4f}'.format(epoch, loss.sum().item()))\n",
        "                pbar.update(1)\n",
        "\n",
        "\n",
        "        return epoch_loss / len(self.train_dataloader)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def val_epoch(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        val_loss = 0\n",
        "        with tqdm(total=len(self.dev_dataloader)) as pbar:\n",
        "          if self.sample_mode=='triplet':\n",
        "            for i, (anchor, positive, negative, class_label) in enumerate(self.dev_dataloader):\n",
        "                anchor = anchor.to(self.device)\n",
        "                anchor_emb = self.model(anchor)\n",
        "\n",
        "                loss = torch.tensor(0.0).to(self.device)\n",
        "                for neg in negative:\n",
        "                    neg = neg.to(self.device)\n",
        "                    negative_emb = self.model(neg)\n",
        "                    for pos in positive:\n",
        "                      pos = pos.to(self.device)\n",
        "                      positive_emb = self.model(pos)\n",
        "\n",
        "                      loss += self.loss(anchor_emb, positive_emb, negative_emb, \"eval\")\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "          if self.sample_mode=='arcface':\n",
        "            for i, (anchor, class_label) in enumerate(self.dev_dataloader):\n",
        "                anchor = anchor.to(self.device)\n",
        "                class_label = class_label.to(self.device)\n",
        "\n",
        "                anchor_emb = self.model(anchor)\n",
        "\n",
        "                loss = self.loss(anchor_emb, class_label).sum()\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        return val_loss / len(self.dev_dataloader)\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            loss = self.train_epoch(epoch)\n",
        "            print(f'Epoch {epoch} - loss {loss}')\n",
        "            wandb.log({\"train_epoch_loss\": loss})\n",
        "\n",
        "            val_loss = self.val_epoch()\n",
        "            print(f'Epoch {epoch} - validation loss {val_loss}')\n",
        "            wandb.log({\"val_loss\": val_loss})\n",
        "#            self.eval()\n",
        "            self._write_checkpoint(val_loss, best_val_loss)\n",
        "\n",
        "    def _optimizer_step(self):\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def _write_checkpoint(self, val_loss, best_val_loss):\n",
        "        if self.save_best:\n",
        "            if val_loss < best_val_loss:\n",
        "                self.model.save(self.model_dir)\n",
        "\n",
        "        else:\n",
        "            save_dir = os.path.join(self.experiment_dir, f'loss_{val_loss}')\n",
        "            os.makedirs(save_dir)\n",
        "            self.model.save(save_dir)\n"
      ],
      "metadata": {
        "id": "5sNi92onUwkH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "l3pZsrfkuqIn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_tens(x):\n",
        "    for i in range(len(x[0])):\n",
        "        yield torch.stack(list(map(itemgetter(i), x)))"
      ],
      "metadata": {
        "id": "M3qaS2pRufkD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from random import choices\n",
        "\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def parce_dataset(source_path: str, dest_path: str, val_size: float = 0.2) -> None:\n",
        "    source_paths = [path for path in os.listdir(source_path) if path not in ['test', 'train', 'val', '.DS_Store']]\n",
        "\n",
        "    for directory in source_paths:\n",
        "        directory_path = os.path.join(source_path, directory)\n",
        "\n",
        "        for file in os.listdir(directory_path):\n",
        "            part = 'val' if choices([0, 1], [1 - val_size, val_size])[0] else 'train'\n",
        "            destination_dir = os.path.join(dest_path, part)\n",
        "\n",
        "            src_path, move_path = os.path.join(directory_path, file), os.path.join(destination_dir, file)\n",
        "\n",
        "            shutil.move(src_path, move_path)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    anchor = torch.stack(list(map(lambda x: x[0], batch)))\n",
        "    positive = list(to_tens(list(map(lambda x: x[1], batch))))\n",
        "    negative = list(to_tens(list(map(lambda x: x[2], batch))))\n",
        "    target = torch.tensor(list(map(lambda x: x[3], batch)))\n",
        "\n",
        "    return [anchor, positive, negative, target]\n",
        "\n",
        "def collate_fn_arcface(batch):\n",
        "    anchor = torch.stack(list(map(lambda x: x[0], batch)))\n",
        "    target = torch.tensor(list(map(lambda x: x[1], batch)))\n",
        "\n",
        "    return [anchor, target]\n",
        "\n",
        "\n",
        "def cosine_similarity(emb1: np.ndarray, emb2: np.array) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Batched cosine similarity for normalized vectors.\n",
        "    :param emb1: (n, dim)\n",
        "    :param emb2: (n, dim)\n",
        "    :return: (n)\n",
        "    \"\"\"\n",
        "\n",
        "    return np.sum(emb1 * emb2, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "QhIKY2QaUy-y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Ghv3iYfRVNHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_argus():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--dataset-dir', default='/content/dataset/train')\n",
        "    parser.add_argument('--val-dataset-dir', default='/content/dataset/val')\n",
        "    parser.add_argument('--test-dataset-dir', default='/content/dataset/test')\n",
        "    parser.add_argument('--checkpoint-dir', default='/content/drive/MyDrive/experiments', help='Checkpoint directory')\n",
        "    parser.add_argument('--model-path', default='/content/drive/MyDrive/enet_b0_8_best_vgaf.pt',\n",
        "                        help='Model directory')\n",
        "    parser.add_argument('--model-class', default='ImageEmbedder', help='')\n",
        "\n",
        "    parser.add_argument('--epochs', type=int, default=5, help='Number of epochs for training')\n",
        "    parser.add_argument('--batch-size', type=int, default=8, help='Number of examples for each iteration')\n",
        "    parser.add_argument('--accumulate-batches', type=int, default=1, help='Number of batches to accumulate')\n",
        "    parser.add_argument('--learning-rate', type=float, default=1e-4, help='Learning rate')\n",
        "    parser.add_argument('--optim-betas', type=list, nargs='+', default=[0.9, 0.999], help='Optimizer betas')\n",
        "    parser.add_argument('--weight-decay', type=float, default=0.01, help='Optimizer weight decay')\n",
        "    parser.add_argument('--threshold_chooser', type=str, default='accuracy',\n",
        "                        help='Could be either max_f1, eer, accuracy')\n",
        "\n",
        "    parser.add_argument('--loss', type=str, default='TripletLoss', help='Could be either ArcFaceLoss or TripletLoss')\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=1004, help='Random seed value')\n",
        "    parser.add_argument('--checkpoint-iter', type=int, default=5000, help='Eval and checkpoint frequency.')\n",
        "    parser.add_argument('--scale-scores', type=bool, default=True,\n",
        "                        help='Scale cosine similarity to [0, 1] for a better score interpretability')\n",
        "    parser.add_argument('--device', default='cuda', help='Device to use for training: cpu or cuda')\n",
        "\n",
        "    return parser.parse_args(args=[])\n"
      ],
      "metadata": {
        "id": "pwU0Vsp6U32f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_argus()"
      ],
      "metadata": {
        "id": "uLHKTF5T2RxP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"Emotions_Recognition\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": args.learning_rate,\n",
        "    \"architecture\": args.model_path,\n",
        "    \"loss\": args.loss\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "48m3xwtm1f65",
        "outputId": "aeffb444-3d2a-40ae-fcda-f8ceddf3c109"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m412549\u001b[0m (\u001b[33mnotn3ss_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240119_185309-sa4ygy0f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/notn3ss_team/Emotions_Recognition/runs/sa4ygy0f' target=\"_blank\">young-deluge-62</a></strong> to <a href='https://wandb.ai/notn3ss_team/Emotions_Recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/notn3ss_team/Emotions_Recognition' target=\"_blank\">https://wandb.ai/notn3ss_team/Emotions_Recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/notn3ss_team/Emotions_Recognition/runs/sa4ygy0f' target=\"_blank\">https://wandb.ai/notn3ss_team/Emotions_Recognition/runs/sa4ygy0f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/notn3ss_team/Emotions_Recognition/runs/sa4ygy0f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ef83ec6d990>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.seed is not None:\n",
        "    set_all_seeds(args.seed)\n",
        "\n",
        "model = eval(args.model_class)(\n",
        "    model_path=args.model_path,\n",
        "    device=args.device\n",
        ")\n",
        "\n",
        "train_dataset = EmotionsDataset(\n",
        "    dataset_dir=args.dataset_dir\n",
        ")\n",
        "dev_dataset = EmotionsDataset(\n",
        "    dataset_dir=args.val_dataset_dir,\n",
        ")\n",
        "test_dataset = EmotionsDataset(\n",
        "    dataset_dir=args.test_dataset_dir,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=6,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=False, num_workers=6,\n",
        "                            collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=6,\n",
        "                              collate_fn=collate_fn)\n",
        "\n",
        "#Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n",
        "\n",
        "loss = eval(args.loss)(\n",
        "    emb_size=512,\n",
        "    num_classes=8,\n",
        "    device=args.device\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=args.learning_rate,\n",
        "    betas=args.optim_betas,\n",
        "    weight_decay=args.weight_decay\n",
        ")\n",
        "\n",
        "if args.loss == 'ArcFaceLoss':\n",
        "  optimizer_arc = torch.optim.AdamW(\n",
        "    loss.parameters(),\n",
        "    lr=args.learning_rate,\n",
        "    betas=args.optim_betas,\n",
        "    weight_decay=args.weight_decay\n",
        "  )\n",
        "else:\n",
        "  optimizer_arc = None\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    len(train_dataset) * args.epochs / args.batch_size,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "trainer = EmotionsTrainer(\n",
        "    model=model,\n",
        "    checkpoint_dir=args.checkpoint_dir,\n",
        "    train_dataloader=train_dataloader,\n",
        "    dev_dataloader=dev_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    optimizer_arc=optimizer_arc,\n",
        "    scheduler=scheduler,\n",
        "    loss=loss,\n",
        "    device=args.device\n",
        ")\n",
        "\n",
        "trainer.train(args.epochs)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Tx4ZLqVCWI",
        "outputId": "cc08e81c-ef50-4ce8-cef1-663b4be8cf89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 - current loss: 34.8152:   1%|          | 26/2153 [00:52<1:07:24,  1.90s/it]"
          ]
        }
      ]
    }
  ]
}